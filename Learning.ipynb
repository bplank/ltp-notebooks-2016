{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this notebook you should:\n",
    "* know where the weights come from\n",
    "* why we need non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feed-forward neural network\n",
    "Recall our network <img src=\"pics/nn.png\"> \n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where do the weights come from? \n",
    "\n",
    "We can apply a learning algorithm called **backprob** to learn (or estimate) these weights. But before we get there, lets first look at an intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to adjust the weights so that *a small change in the output should have a small effect in the output*.\n",
    "However, with a simple perceptron a small change might often have a large effect. Remember, the decision function for the perceptron is a threshold, this can be seen as a **step function**: <img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ac/HardLimitFunction.png\" width=300> \n",
    "\n",
    "It is 0 for everything below 0 and 1 for positive outputs. If you are already close to the threshold, a small change might have a large effect.\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz8.png\">\n",
    "\n",
    "For another reason that we will see later, we will not use simple thresholding, i.e., a **step function**, but rather a smoother function like the **sigmoid** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's $g$? A sigmoid neuron\n",
    "\n",
    "A commonly used activation function $g$ is the sigmoid **sigmoid function** (aka logistic sigmoid function). A sigmoid neuron gets inputs and weights them by $W$, then it applies the **non-linearity** $g$.\n",
    "\n",
    "$$y = g_{sigmoid}(x \\cdot w + b)$$\n",
    "\n",
    "where $$g_{sigmoid} = \\sigma(z) = \\frac{1}{1 + e ^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXUQkhRTuIiIppHAXEUmh/wdoiPRuXpMHSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d7043c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "## credits: http://squall0032.tumblr.com/post/77300791096/plotting-a-sigmoid-function-using\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "plt.plot(x,sig)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume now our network above contains sigmoid neurons. \n",
    "$$NN_{MLP1}(\\mathbf{x})=g_{sigmoid}(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(CREDITS: The following has been taken from AJ and ZA's tutorial):\n",
    "We'll start with a hypothetical (xkcd-style).\n",
    "\n",
    "<img src=\"https://what-if.xkcd.com/imgs/whatif-logo.png\">\n",
    "\n",
    "### What if all the non-linearities in an NN suddenly vanished?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, lets simply ignore the bias term to simplfiy our notation. \n",
    "\n",
    "A neural network with an input layer, a middle layer, and an output layer computes the following:\n",
    "\n",
    "$$\\mathbf{y} = g(W^{(0)}g(W^{(1)}g(W^{(0)}\\mathbf{x})))$$\n",
    "\n",
    "$g$ is a non-linearity, which could be different for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change $g$ to a linear function (e.g. a scaling factor), it can simply be multiplied into the weights matrices. Below we assume that $g = 1$, which allows us to simplify the expression:\n",
    "\n",
    "$$\\mathbf{y} = (W^{(0)}(W^{(1)}(W^{(0)}\\mathbf{x})))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since matrix multiplication is associative:\n",
    "\n",
    "$$A(BC) = (AB)C,$$\n",
    "\n",
    "we can get rid of the brackets altogether:\n",
    "\n",
    "$$\\mathbf{y} = W^{(0)}W^{(1)}W^{(0)}\\mathbf{x}.$$\n",
    "\n",
    "The series of linear transformations can be summarized in a single transformation matrix :\n",
    "\n",
    "$$T = W^{(0)}W^{(1)}W^{(0)}.$$\n",
    "\n",
    "And so the prediction of the neural network becomes:\n",
    "\n",
    "$$\\mathbf{y} = T\\mathbf{x}.$$\n",
    "\n",
    "The effective number of parameters in the now non non-linear neural network is $|\\mathbf{y}| \\times |\\mathbf{x}|$, which is precisely the same as a standard linear model.\n",
    "\n",
    "I.E, the non-linearities are crucial!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have called 'compute' before is usually referred to as the forward pass in the neural network. It is going from the inputs through the layers to the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# forward-pass of a 3-layer neural network:\n",
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
    "x = np.random.randn(3) # random input vector of three numbers (1x3) \n",
    "W1 = np.zeros((3,4))   # Weights (3x4)\n",
    "W2 = np.zeros((4,4))   # Weights (4x4)\n",
    "W3 = np.zeros((4,1))   # Weights (4x1)\n",
    "b1 = np.zeros((1,4))\n",
    "b2 = np.zeros((1,4))\n",
    "b3 = np.zeros((1,4))\n",
    "h1=f(np.dot(x,W1)+b1) # calculate the activations of the first hidden layer (1x4) - linear transformation followed by non-linearity!\n",
    "h2=f(np.dot(h1,W2)+b2) # calculate the activations of the second hidden layer (1x4)\n",
    "out = np.dot(W3, h2) + b3 # output neuron (1x1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass (derivates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High level idea**: Calculate partial derivatives with respect to each parameter in the network.  The **backpropagation algorithm** is a way of computing gradients of expressions through recursive application of the **chain rule**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a derivative?\n",
    "\n",
    "A derivative gives us a linear approximation of the function at a specific point.\n",
    "\n",
    "So given a function $f$, its derivate at point $a$, $m = f'(a)$, we can approximate the original function as \n",
    "\n",
    "$$g(x) = f(a) + m(a - x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to estimate the impact of a change in a parameter with respect to the final output of the network. It is especially useful if we make the network compute a **scalar loss** during training, i.e. have a $1 \\times 1$ output node that measures how much we **dislike** the current prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute error signal $\\delta_j = a_j - y$ for each output node, then propagate this back through the network. \n",
    "\n",
    "We'll return to this when we see computational graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video: [Andrew Ng's video on Coursera](https://class.coursera.org/ml-003/lecture/51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonly-used activation functions\n",
    "Tanh: <img src=\"http://cs231n.github.io/assets/nn1/tanh.jpeg\">\n",
    "Sigmoid: <img src=\"http://cs231n.github.io/assets/nn1/sigmoid.jpeg\">\n",
    "ReLu: <img src=\"http://cs231n.github.io/assets/nn1/relu.jpeg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we will have problems with multiple classes. In deep learning, each class is usually represented as a **one-hot** vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What is y?\n",
    "<img src=\"pics/multiclass.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is x?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning we usually work with dense representations. Each feature is a vector of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) sparse representation vs b) dense representation**\n",
    "\n",
    "<img src=\"pics/sparsevsdense.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* More details in [Michael Nielsen's book chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "* Yoav Goldberg's tutorial: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* AJ's [simplest_nn](https://github.com/andersjo/simplest_nn)\n",
    "* CS231n [notes](http://cs231n.github.io/optimization-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
