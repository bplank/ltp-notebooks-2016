{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know what a neural network is\n",
    "* understand its basic building blocks\n",
    "* understand why we need non-linearities\n",
    "* connect different views on neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A neural network is computational model that has slightly different meanings in different communitites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Cognitive science view**: a computational model of the brain consisting of artificial neural perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Machine Learning view**: \n",
    "   * **Linear algebra view**: a network of perceptron-like nodes, i.e., a set of matrix multiplication operations\n",
    "   * **Graph theory view**: a computational graph model (with automatic differentiation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As the name already suggests, a neural network is a network. It can be seen as a model that is build up from basic building blocks. Lets first look at one such building block, for instance, a single perceptron. \n",
    "\n",
    "<img src=\"pics/lego.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From biological neurons to artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To get started, I will first introduce a type of artificial neuron the **perceptron**. It was introduced with the well-known perceptron algorithm by Rosenblatt (1957), inspired by earlier work on McCulloch-Pitts to model neurons in the brain. In layman's terms, a neuron gets information through dendrites and if enough information is accumulated the neuron 'fires' and send information down the axon: \n",
    "\n",
    "<img src=\"pics/neuron.jpg\" width=\"350\" style=\"float: left\"><img src=\"pics/neuron-simple.png\" width=\"350\">\n",
    "\n",
    "Thus neural networks are biologically inspired. (But its overly simplistic..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does the perceptron work?\n",
    "The basic perceptron gets **inputs** $x_1,..,x_n$ and produces an **output** $y$. It does so by **weighting** the inputs by $w_1,..,w_n$, sums up the weighted intputs and sends this weighted sum through an **activation function** $\\sigma$ doto see if the neuron \"fires\". That is, if the weighted sum is above some **threshold** it will output 1, otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, the perceptron is formulated as: \n",
    "\n",
    "$y = \\sigma(\\sum_{j=1}^d w_{kj} x_j )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can visualize the perceptron as (for a given perceptron node $k$): <img src=\"pics/perceptron.png\" width=400> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the perceptron $\\sigma$ is a **threshold** function. Intuitively, the perceptron only fires if the weighted sum is above some threshold. We can formulize this intuition as:\n",
    "\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (\\sum_j w_j x_j) > threshold\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets rewrite the equation of the perceptron. First, notice that $\\sum_{j=1} w_{j} x_j $ is the **dot product** of the weights and input, and can be written as: \n",
    "\n",
    "$$\\sum_{j=1} w_{j} x_j = \\vec{w} \\cdot \\vec{x}$$ where $\\vec{w}$ and $\\vec{x}$ are now vectors. If it is clear from context we avoid the explicit vector notation and simply write: $w \\cdot x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second, we will move the threshold inside the equation by introducing $b$ the bias term $b=-threshold$. Using these two changes, the equation rewrites as:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Suppose we have a perceptron with two inputs, weights -2 and -2 and bias term 3. This is illustrated as: <img src=\"pics/nand-graph.png\">\n",
    "What function does this simple perceptron compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def compute(x1, x2):\n",
    "    a = x1*-2 + x2*-2 + 1 * 3\n",
    "    if a > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "x1=0\n",
    "x2=0\n",
    "print(compute(x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectorization\n",
    "Instead of this cumbersome notation, lets use vectorization. Now we represent our input instances as vectors, and the entire data as a matrix. Also the weights are a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]])\n",
    "def compute(input_matrix):\n",
    "    W = [-2,-2]\n",
    "    b = 3\n",
    "    a = np.dot(input_matrix,W) + b\n",
    "    return [1 if elem > 0 else 0 for elem in a]\n",
    "labels=compute(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beautiful! Now we have a perceptron that models the NAND logical function. That is, it return 1 only if both inputs are active (not AND)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize the example by looking at where the input vectors are in the space and which label they get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"pics/nand-plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the perceptron formula, what does it resemble?\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Right, it's the equation of a line! To be precise, since the inputs have usually more than 2 dimensions it is actually a **hyperplane**. Dry to imagine the line in our NAND example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise:** try to modify the parameters (weights and bias) of our network. Can you get another logical function? Here is a little help that plots the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10688dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "colors=['r','b']\n",
    "types=['x','o']\n",
    "for i, lab in enumerate(labels):\n",
    "    plt.plot(inputs[i,0], inputs[i,1], types[lab], color=colors[lab])\n",
    "plt.axis([-0.5, 1.5, -0.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability\n",
    "\n",
    "The perceptron is a **linear** classifier. Now this should be clear from the formula. Here are examples of problem which are linearly separable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Linearly separable?\n",
    "How have a look at the following two examples. Are they linearly separable? (hint: Which logical functions do they represent?) \n",
    "<img src=\"pics/linearq.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-8da4f70dcf3e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8da4f70dcf3e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    left: OR, right: XOR\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "left: OR, right: XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Right. And this is a limitation of the perceptron. If the data is not linearly separable, the perceptron has a hard time. So what can we do about it? There are tricks to make the perceptron work in such cases, but usually you will move to a model with higher **representational capacity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/separability.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Neural Networks\n",
    "\n",
    "* **non-linearity**\n",
    "* **representational power** (can represent any function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A neural network is a network of node. It has **input** nodes, **output node(s)** and usually **hidden nodes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Below is a visualization of a neural network. The green nodes are the inputs, the blue nodes are **hidden nodes** and the last *layer* is the **output** layer. How many input, hidden and output nodes does this network have?\n",
    "\n",
    "A feedforward neural network:\n",
    "<img src=\"pics/nn.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or another visualization: <img src=\"http://neuralnetworksanddeeplearning.com/images/tikz11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Such a basic neural network is also called:\n",
    "* **feedforward neural network**\n",
    "* **multi-layer perceptron** (MLP) (for some odd historical reasons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets look at a more detailed example. The network  can be formulized as:\n",
    "<img src=\"pics/nn.png\" width=200> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "there $g$ is a non-linearity/activation function. We will come back to this later. For now, discuss with your neighbor: what are all the terms in the formula above, and how can you connect them to the picture above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A feedforward neural network with 2 hidden layers:\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The MLP2 is illustrated here (vertically). \n",
    "<img src=\"pics/nn_vertical.png\" width=300>\n",
    "It is a bit cumbersome to see, so lets break the formula \n",
    "\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n",
    "\n",
    "down into parts:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathbf{h^1}=g^1(\\mathbf{xW^1+b^1})$$\n",
    "$$\\mathbf{h^2}=g^2(\\mathbf{h_1W^2+b^2})$$\n",
    "$$NN_{MLP2}(\\mathbf{x})= \\mathbf{h^2}\\mathbf{W^3}+\\mathbf{b^3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Voila! Now we have a wonderful description of a neural network, both graphically and algebraically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "* More details in [Michael Nielsen's book chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "* Yoav Goldberg's tutorial: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "break"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
