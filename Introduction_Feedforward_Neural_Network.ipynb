{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know what a neural network is\n",
    "* understand its basic building blocks\n",
    "* understand why we need non-linearities\n",
    "* connect different views on neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A neural network is computational model that has slightly different meanings in different communitites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Cognitive science view**: a computational model of the brain consisting of artificial neural perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Machine Learning view**: \n",
    "   * **Linear algebra view**: a network of perceptron-like nodes, i.e., a set of matrix multiplication operations\n",
    "   * **Graph theory view**: a computational graph model (with automatic differentiation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As the name already suggests, a neural network is a network. It can be seen as a model that is build up from basic building blocks. Lets first look at one such building block, for instance, a single perceptron. \n",
    "\n",
    "<img src=\"pics/lego.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From biological neurons to artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To get started, I will first introduce a type of artificial neuron the **perceptron**. It was introduced with the well-known perceptron algorithm by Rosenblatt (1957), inspired by earlier work on McCulloch-Pitts to model neurons in the brain. In layman's terms, a neuron gets information through dendrites and if enough information is accumulated the neuron 'fires' and send information down the axon: \n",
    "\n",
    "<img src=\"pics/neuron.jpg\" width=\"350\" style=\"float: left\"><img src=\"pics/neuron-simple.png\" width=\"350\">\n",
    "\n",
    "Thus neural networks are biologically inspired. (But its overly simplistic..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does the perceptron work?\n",
    "The basic perceptron gets **inputs** $x_1,..,x_n$ and produces an **output** $y$. It does so by **weighting** the inputs by $w_1,..,w_n$, sums up the weighted intputs and sends this weighted sum through an **activation function** $\\sigma$ doto see if the neuron \"fires\". That is, if the weighted sum is above some **threshold** it will output 1, otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, the perceptron is formulated as: \n",
    "\n",
    "$y = \\sigma(\\sum_{j=1}^d w_{kj} x_j )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can visualize the perceptron as (for a given perceptron node $k$): <img src=\"pics/perceptron.png\" width=400> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the perceptron $\\sigma$ is a **threshold** function. Intuitively, the perceptron only fires if the weighted sum is above some threshold. We can formulize this intuition as:\n",
    "\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (\\sum_j w_j x_j) > threshold\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets rewrite the equation of the perceptron. First, notice that $\\sum_{j=1} w_{j} x_j $ is the **dot product** of the weights and input, and can be written as: \n",
    "\n",
    "$$\\sum_{j=1} w_{j} x_j = \\vec{w} \\cdot \\vec{x}$$ where $\\vec{w}$ and $\\vec{x}$ are now vectors. If it is clear from context we avoid the explicit vector notation and simply write: $w \\cdot x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second, we will move the threshold inside the equation by introducing $b$ the bias term $b=-threshold$. Using these two changes, the equation rewrites as:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Suppose we have a perceptron with two inputs, weights -2 and -2 and bias term 3. This is illustrated as: <img src=\"pics/nand-graph.png\">\n",
    "What function does this simple perceptron compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def compute(x1, x2):\n",
    "    a = x1*-2 + x2*-2 + 1 * 3\n",
    "    if a > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "x1=0\n",
    "x2=0\n",
    "print(compute(x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectorization\n",
    "Instead of this cumbersome notation, lets use vectorization. Now we represent our input instances as vectors, and the entire data as a matrix. Also the weights are a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]])\n",
    "def compute(input_matrix):\n",
    "    W = [-2,-2]\n",
    "    b = 3\n",
    "    a = np.dot(input_matrix,W) + b\n",
    "    return [1 if elem > 0 else 0 for elem in a]\n",
    "    \n",
    "print(compute(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beautiful! Now we have a perceptron that models the NAND logical function. That is, it return 1 only if both inputs are active (not AND)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize the example by looking at where the input vectors are in the space and which label they get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"pics/nand-plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the perceptron formula, what does it resemble?\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Right, it's the equation of a line! To be precise, since the inputs have usually more than 2 dimensions it is actually a **hyperplane**. Dry to imagine the line in our NAND example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability\n",
    "\n",
    "The perceptron is a **linear** classifier. Now this should be clear from the formula. Here are examples of problem which are linearly separable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Linearly separable?\n",
    "How have a look at the following two examples. Are they linearly separable? (hint: Which logical functions do they represent?) \n",
    "<img src=\"pics/linearq.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "left: OR, right: XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Right. And this is a limitation of the perceptron. If the data is not linearly separable, the perceptron has a hard time. So what can we do about it? There are tricks to make the perceptron work in such cases, but usually you will move to a model with higher **representational capacity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/separability.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Neural Networks\n",
    "\n",
    "* **non-linearity**\n",
    "* **representational power** (can represent any function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A neural network is a network of node. It has **input** nodes, **output node(s)** and usually **hidden nodes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Below is a visualization of a neural network. The green nodes are the inputs, the blue nodes are **hidden nodes** and the last *layer* is the **output** layer. How many input, hidden and output nodes does this network have?\n",
    "\n",
    "A feedforward neural network:\n",
    "<img src=\"pics/nn.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or another visualization: <img src=\"http://neuralnetworksanddeeplearning.com/images/tikz11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Such a basic neural network is also called:\n",
    "* **feedforward neural network**\n",
    "* **multi-layer perceptron** (MLP) (for some odd historical reasons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets look at a more detailed example. The network  can be formulized as:\n",
    "<img src=\"pics/nn.png\" width=200> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "there $g$ is a non-linearity/activation function. We will come back to this later. For now, discuss with your neighbor: what are all the terms in the formula above, and how can you connect them to the picture above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A feedforward neural network with 2 hidden layers:\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The MLP2 is illustrated here (vertically). \n",
    "<img src=\"pics/nn_vertical.png\">\n",
    "It is a bit cumbersome to see, so lets break the formula \n",
    "\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n",
    "\n",
    "down into parts:\n",
    "$$\\mathbf{h^1}=g^1(\\mathbf{xW^1+b^1})$$\n",
    "$$\\mathbf{h^2}=g^2(\\mathbf{h_1W^2+b^2})$$\n",
    "$$NN_{MLP2}(\\mathbf{x})= \\mathbf{h^2}\\mathbf{W^3}+\\mathbf{b^3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Voila! Now we have a wonderful description of a neural network, both graphically and algebraically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "* More details in [Michael Nielsen's book chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "* Yoav Goldberg's tutorial: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "break"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
