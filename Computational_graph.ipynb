{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Graph view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* understand the computational graph abstraction and how it relates to backprob\n",
    "* know the biggest difference in traditional and deep learning feature representations  [dense vs one-hot representations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen how a neural network can be formalized, both algebraically and graphically. we can think of a feed-forward NN as a function $NN(\\mathbf{x})$: $$y= NN(\\mathbf{x}) $$\n",
    "\n",
    "with:\n",
    "input: $\\mathbf{x}$ (vector with $d_{in}$ dimensions)\n",
    "\n",
    "output: $\\mathbf{y}$ (output with $d_{out}$ classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, we have seen last time that the following network can be formalized as:\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Where do the weights come frome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's an **optimization** problem. We want to find the weights that \"work best\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/mountains_at_home.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training a Neural Network: Ingredients\n",
    "\n",
    "* we need to **define what \"works best\" means**\n",
    "* we need **a way to change the model (parameters)** to get closer to a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining what works best ~ how close we are: Loss\n",
    "\n",
    "Measures how 'far off' we are from true solution:\n",
    "\n",
    "$$L(\\mathbf{\\hat{y}},\\mathbf{y})$$\n",
    "\n",
    "For multi-class classification the **cross-entropy** is a commonly used loss function: \n",
    "\n",
    "$$L_{crossentropy}(\\mathbf{\\hat{y}},\\mathbf{y})= - log(\\hat{y}_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to get closer to a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 1:** random guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 2:** start with some random initial parameters (weights), and randomly adjust them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 3:** follow the gradient: analytical method to find the best direction along which we should change our weight vector: **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6d/Error_surface_of_a_linear_neuron_with_two_input_weights.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To sum up: Ingredients for training a Neural Network\n",
    "\n",
    "* we need to define what \"works best\" means \n",
    "    $\\rightarrow$ minimize some **loss**\n",
    "* we need a way to change the parameters to get closer to a good model\n",
    "    $\\rightarrow$ **minimize loss using a gradient-based method: gradient descent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Intuitively, training a neural networks involves the following steps:\n",
    "\n",
    "* compute the gradient of the loss function with respect to the parameters\n",
    "* move the parameters in the negative direction of the gradient to decrease the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Skeleton of gradient descent:\n",
    "    \n",
    "**Input**: training set, loss function $L$\n",
    "\n",
    "Repeat for number of iterations (**epochs**): \n",
    " \n",
    "* compute loss on data: $L(X,Y)$\n",
    "* compute gradients: $\\mathbf{g} = L(X,Y)$ with respect to $w$\n",
    "* move parameters in direction of gradient: $w \\pm -\\eta \\mathbf{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to get an intuitive understanding of the **backpropagation** algorithm. Backprob is a way of computing **gradients** of expressions through applying the **chain rule**. But before we get into details of gradients etc, lets introduce the **computational graph abstraction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent our neural network as **computational graph**: nodes are operations, gray boxes are parameters.\n",
    "\n",
    "<img src=\"pics/yg-compgraph1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This corresponds to the neural network we have seen before:\n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why computational graph?\n",
    "\n",
    "It helps us to understand the flow of parameters in the model. \n",
    "\n",
    "What do we need to compute? the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "**What's a gradient?**: A vector of partial derivatives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Recall: the **derivative** \n",
    "\n",
    "A derivative gives us a linear approximation of the function at a specific point. Intuitively, the derivative indicates the rate of change of a function $f$ with respect to a variable $x$ (surrounding the region around point $h$):\n",
    "\n",
    "    \n",
    "<img src=\"http://www.intuitive-calculus.com/images/what-is-a-derivative-4.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "Our functions are not just functions of single parameters, but of a lot of parameters. We are interested in finding all partial derivatives, i.e. the gradient. For our example function: \n",
    "\n",
    "the gradient is:\n",
    "\n",
    " $$\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y,x]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example: gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets take a simple example of a function: $$f(x) = (x * y)$$ (or simply): $$f(x)=xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to calculate the gradient, the vector of partial derivatives (how much does the function change wrt the parameters x and y): $$\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The partial derivatives of this function are:\n",
    "\n",
    "$$f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## f(x) = x * y\n",
    "# lets take some numbers \n",
    "x = 4\n",
    "y = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n"
     ]
    }
   ],
   "source": [
    "## forward pass (function application)\n",
    "f = x * y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "## the derivative of each variable tells us the sensitivity of the whole expression on its value. \n",
    "## for instance, take the partial derivative of f wrt y:\n",
    "df_dy = x  # it's simply y \n",
    "print(x) # this means if we increase the y by a tiny amount, the whole function would increase by this amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "## similarly, the partial derivative of f w.r.t. x is:\n",
    "df_dx = y\n",
    "print(y)  # changing x by some small amount would make the whole expression decrease (negative sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**the derivative on each variable tells you the sensitivity of the whole expression on its value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2: computational graph - compound expression\n",
    "\n",
    "(Thanks to lecture notes by Fei-Fei, Karphaty and Johnson, cf: http://cs231n.github.io/optimization-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets take the function: $$f(x) = (x + y) + z$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(forward pass):\n",
    "<img src=\"pics/k1.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k2.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k3.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k4.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k5.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k6.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k7.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k8.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k9.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k10.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k11.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k12.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/k13.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/graph.png\"> Slide by Fei-Fei, Karpathy and Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using the chain rule\n",
    "\n",
    "* we can compute the gradients of the loss along the backward path in our computational graph\n",
    "* once we know the gradients: we know how much we should change our parameters (in negative direction of gradients, as we want to minimize the loss): $w \\pm -\\eta \\mathbf{g}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###### Gradient descent\n",
    "\n",
    "Repeat for number of iterations (**epochs**): \n",
    "* compute loss on data: $L(X,Y)$\n",
    "* compute gradients: $\\mathbf{g} = L(X,Y)$ with respect to $w$\n",
    "* move parameters in direction of gradient: $w \\pm -\\eta \\mathbf{g}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### In practice:\n",
    "\n",
    "* **stochastic gradient descent** (online learning)\n",
    "* **mini-batches** (use a small subset of training instances) (minibatch size)\n",
    "* **further hyperparameters**: learning rate $\\eta$ (how big a step we take), number of epochs (how often we go over training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* computation graph for f(a,b) = (a*b+1)(a*b+2)\n",
    "* sharing of a*b\n",
    "* computation graph for 1-layer MLP\n",
    "* explain notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 6: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Fei-Fei, Karpathy and Johnson's lecture notes: http://cs231n.github.io/optimization-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
