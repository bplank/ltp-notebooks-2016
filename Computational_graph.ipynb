{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Graph view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* understand the computational graph abstraction and how it relates to backprob\n",
    "* know the biggest difference in traditional and deep learning feature representations  [dense vs one-hot representations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen how a neural network can be formalized, both algebraically and graphically. we can think of a feed-forward NN as a function $NN(\\mathbf{x})$: $$y= NN(\\mathbf{x}) $$\n",
    "\n",
    "with:\n",
    "input: $\\mathbf{x}$ (vector with $d_{in}$ dimensions)\n",
    "\n",
    "output: $\\mathbf{y}$ (output with $d_{out}$ classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, we have seen last time that the following network can be formalized as:\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Where do the weights come frome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Its an **optimization** problem. We want to find the weights that \"work best\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/mountains_at_home.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training a Neural Network: Ingredients\n",
    "\n",
    "* we need to **define what \"works best\" means**\n",
    "* we need **a way to change the model (parameters)** to get closer to a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining what works best ~ how close we are: Loss\n",
    "\n",
    "Measures how far 'off' we are from true solution:\n",
    "\n",
    "$$L(\\mathbf{\\hat{y}},\\mathbf{y})$$\n",
    "\n",
    "For multi-class classification the **cross-entropy** is a commonly used loss function: \n",
    "\n",
    "$$L_{crossentropy}(\\mathbf{\\hat{y}},\\mathbf{y})= - log(\\hat{y}_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6d/Error_surface_of_a_linear_neuron_with_two_input_weights.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to get closer to a good model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 1:** random guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 2:** start with some random initial parameters (weights), and randomly adjust them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Strategy 3:** follow the gradient: analytical method to find the best direction along which we should change our weight vector: **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To sum up: Ingredients for training a Neural Network\n",
    "\n",
    "* we need to define what \"works best\" means \n",
    "    $\\rightarrow$ minimize some **loss**\n",
    "* we need a way to change the parameters to get closer to a good model\n",
    "    $\\rightarrow$ **minimize loss using a gradient-based method: gradient descent**\n",
    "\n",
    "<img src=\"pics/optimization.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Intuitively, training a neural networks involves the following steps:\n",
    "\n",
    "* compute the gradient of the loss function with respect to the parameters\n",
    "* move the parameters in the negative direction of the gradient to decrease the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Skeleton of gradient descent:\n",
    "    \n",
    "**Input**: training set, loss function $L$\n",
    "\n",
    "Repeat for number of iterations (**epochs**): \n",
    " \n",
    "* compute loss on data: $L(X,Y)$\n",
    "* compute gradients: $\\mathbf{g} = L(X,Y)$ with respect to $w$\n",
    "* move parameters in direction of gradient: $w \\pm -\\eta \\mathbf{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to get an intuitive understanding of the **backpropagation** algorithm. Backprob is a way of computing **gradients** of expressions through applying the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What's a gradient?**: A vector of partial derivatives. So, in essence we want to calculate partial derivatives with respect to each parameter in the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Recall: the **derivative** \n",
    "\n",
    "A derivative gives us a linear approximation of the function at a specific point. Intuitively, the derivative indicates the rate of change of a function $f$ with respect to a variable $x$ (surrounding the region around point $h$):\n",
    "\n",
    "    \n",
    "<img src=\"http://www.intuitive-calculus.com/images/what-is-a-derivative-4.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a simple example of a function: $$f(x) = (x * y)$$ (or simply): $$f(x)=xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the gradient, the vector of partial derivatives (how much does the function change wrt the parameters x and y): $$\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivatives of this function are:\n",
    "\n",
    "$$f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## f(x) = (x+y)*z\n",
    "# lets take some numbers \n",
    "### src: Example taken from: http://cs231n.github.io/optimization-2/ \n",
    "x = 4\n",
    "y = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n"
     ]
    }
   ],
   "source": [
    "## forward pass (function application)\n",
    "f = x * y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "## the derivative of each variable tells us the sensitivity of the whole expression on its value. \n",
    "## for instance, take the partial derivative of f wrt y:\n",
    "df_dy = x  # it's simply y \n",
    "print(x) # this means if we increase the y by a tiny amount, the whole function would increase by this amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "## similarly, the partial derivative of f w.r.t. x is:\n",
    "df_dx = y\n",
    "print(y)  # changing x by some small amount would make the whole expression decrease (negative sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1080e1da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As summarized in http://cs231n.github.io/optimization-2/:\n",
    "\n",
    "**the derivative on each variable tells you the sensitivity of the whole expression on its value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add computation graph of our tiny example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Example 2: compound expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "Our functions are not just functions of single parameters, but of a lot of parameters. We are interested in finding all partial derivatives, i.e. the gradient. For our example function: \n",
    "\n",
    "the gradient is:\n",
    "\n",
    " $$\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y,x]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* computation graph for f(a,b) = (a*b+1)(a*b+2)\n",
    "* sharing of a*b\n",
    "* computation graph for 1-layer MLP\n",
    "* explain notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/yg-compgraph1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 6: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* http://cs231n.github.io/optimization-2/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
