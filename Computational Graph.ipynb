{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Graph view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this lecture you should:\n",
    "* understand the computational graph abstraction and how it relates to backprob\n",
    "* know what deep learning is\n",
    "* understand traditional and deep learning feature representations (embeddings)   [dense vs one-hot representations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the last lecture we saw feed-forward neural networks. For now, we can think of a feed-forward NN as a function $NN(\\mathbf{x})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y= NN(\\mathbf{x}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: $\\mathbf{x}$ (vector with $d_{in}$ dimensions)\n",
    "\n",
    "output: $\\mathbf{y}$ (output with $d_{out}$ classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how a neural network can be formalized, both algebraically and graphically. $$y= NN(\\mathbf{x}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we have seen last time that the following network can be formalized as:\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do the weights come frome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its an **optimization** problem. You want to find the weights that \"work best\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we do that? \n",
    "\n",
    "* we need to define what \"works best\" means $\\rightarrow$ minimize some **loss**\n",
    "* we need a way to change the parameters to get closer to a good model $\\rightarrow$ **minimize loss over a training set using a gradient-based method**\n",
    "\n",
    "<img src=\"pics/optimization.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, training a neural networks involves the following steps:\n",
    "\n",
    "* compute an estimate of the error over the dataset\n",
    "* compute the gradient with respect to the error\n",
    "* move the parameters in the direction of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton of gradient descent:\n",
    "    \n",
    "**Input**: training set, loss function $L$\n",
    "\n",
    "Repeat for number of iterations (**epochs**): \n",
    " \n",
    "* compute loss on data: $L(X,Y)$\n",
    "* compute gradients: $\\mathbf{g} = L(X,Y) w.r.t. w$\n",
    "* move parameters in direction of gradient: $w = w + \\eta \\mathbf{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get an intuitive understanding of the **backpropagation** algorithm. Backprob is a way of computing **gradients** of expressions through applying the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's a gradient?**: A vector of partial derivatives. So, in essence we want to calculate partial derivatives with respect to each parameter in the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: the **derivative** \n",
    "\n",
    "A derivative gives us a linear approximation of the function at a specific point. Intuitively, the derivative indicates the rate of change of a function $f$ with respect to a variable $x$ (surrounding the region around point $h$):\n",
    "\n",
    "    \n",
    "<img src=\"http://www.intuitive-calculus.com/images/what-is-a-derivative-4.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a simple example of a function: $$f(x) = (x * y)$$ (or simply): $$f(x)=xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the gradient, the vector of partial derivatives (how much does the function change wrt the parameters x and y): $$\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivatives of this function are:\n",
    "\n",
    "$$f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## f(x) = (x+y)*z\n",
    "# lets take some numbers \n",
    "### src: Example taken from: http://cs231n.github.io/optimization-2/ \n",
    "x = 4\n",
    "y = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n"
     ]
    }
   ],
   "source": [
    "## forward pass (function application)\n",
    "f = x * y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "## the derivative of each variable tells us the sensitivity of the whole expression on its value. \n",
    "## for instance, take the partial derivative of f wrt y:\n",
    "df_dy = x  # it's simply y \n",
    "print(x) # this means if we increase the y by a tiny amount, the whole function would increase by this amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "## similarly, the partial derivative of f w.r.t. x is:\n",
    "df_dx = y\n",
    "print(y)  # changing x by some small amount would make the whole expression decrease (negative sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As summarized in http://cs231n.github.io/optimization-2/:\n",
    "\n",
    "**the derivative on each variable tells you the sensitivity of the whole expression on its value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add computation graph of our tiny example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Example 2: compound expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "Our functions are not just functions of single parameters, but of a lot of parameters. We are interested in finding all partial derivatives, i.e. the gradient. For our example function: \n",
    "\n",
    "the gradient is:\n",
    "\n",
    " $$\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y,x]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* computation graph for f(a,b) = (a*b+1)(a*b+2)\n",
    "* sharing of a*b\n",
    "* computation graph for 1-layer MLP\n",
    "* explain notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/yg-compgraph1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 6: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* http://cs231n.github.io/optimization-2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
