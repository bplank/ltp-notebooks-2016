{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks - Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have seen how a neural network can be formalized, both algebraically and graphically, and we saw the computational graph abstraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "<img src=\"pics/yg-compgraph1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, what is the input $\\textbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know about distributional similarity\n",
    "* understand the difference between discrete and continuous/dense feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we go further, lets make a detour and recap: How do we represent a training instance in a traditional classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, recall our example from week 1: training a Logistic Regression classifier for sentiment classification. \n",
    "\n",
    "* Describe in words: what were the features we used? I.e., how did we represent a training instance $\\textbf{x}$?\n",
    "* How can you now describe the entire sentiment training data set as a matrix $X$, i.e.,  what are the rows and columns of $X$? $$ X = \\{\\mathbf{x_1}, ... , \\mathbf{x_n}\\} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data..\n",
      "split data..\n",
      "#train instances: 7996 #test instances: 2666\n",
      "vectorize data..\n",
      "train model..\n",
      "evaluate model..\n",
      "Accuracy: 0.752438109527\n",
      "Majority baseline: 0.491372843211\n"
     ]
    }
   ],
   "source": [
    "__author__ = \"bplank\"\n",
    "\"\"\"\n",
    "Exercise: sentiment classification with logistic regression\n",
    "\n",
    "1) Examine the code. What are the features used?\n",
    "2) What is the distribution of labels in the data?\n",
    "3) Add code to train and evaluate the classifier. What accuracy do you get? What is weird?\n",
    "4) How could you improve the representation of the data?\n",
    "5) Implement cross-validation.\n",
    "\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def load_sentiment_sentences_and_labels():\n",
    "    \"\"\"\n",
    "    loads the movie review data\n",
    "    \"\"\"\n",
    "    ## Q1: What are the features used? Q4: How could you improve the representation of the data?\n",
    "    positive_sentences = [line.strip() for line in open(\"data/rt-polarity.pos\").readlines()]\n",
    "    negative_sentences = [line.strip() for line in open(\"data/rt-polarity.neg\").readlines()]\n",
    "\n",
    "    ## Q2: What is the label distribution?\n",
    "    positive_labels = [1 for sentence in positive_sentences]\n",
    "    negative_labels = [0 for sentence in negative_sentences]\n",
    "\n",
    "    sentences = np.concatenate([positive_sentences,negative_sentences], axis=0)\n",
    "    labels = np.concatenate([positive_labels,negative_labels],axis=0)\n",
    "    \n",
    "    # ADD: randomly shuffle data\n",
    "    seed=12345\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(sentences)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels)\n",
    "    return sentences, labels\n",
    "\n",
    "## read input data\n",
    "print(\"load data..\")\n",
    "sentences, labels = load_sentiment_sentences_and_labels()\n",
    "# Q: What accuracy do you get when you run the code? What is weird? (if we don't shuffle)\n",
    "print(\"split data..\")\n",
    "split_point = int(0.75*len(sentences))\n",
    "X_train, X_test = sentences[:split_point], sentences[split_point:]\n",
    "y_train, y_test = labels[:split_point], labels[split_point:]\n",
    "\n",
    "print(\"#train instances: {} #test instances: {}\".format(len(X_train),len(X_test)))\n",
    "assert(len(X_train)==len(y_train))\n",
    "assert(len(X_test)==len(y_test))\n",
    "\n",
    "## Explain to your neighbor, what happens here?\n",
    "majority_label = Counter(labels).most_common()[0][0]\n",
    "majority_prediction = [majority_label for label in y_test]\n",
    "\n",
    "print(\"vectorize data..\")\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = Pipeline( [('vec', vectorizer),\n",
    "                        ('clf', LogisticRegression())] )\n",
    "\n",
    "### Q2: add code to train and evaluate your classifier\n",
    "print(\"train model..\")\n",
    "## your code here:\n",
    "classifier.fit(X_train, y_train)\n",
    "##\n",
    "print(\"evaluate model..\")\n",
    "## your code here:\n",
    "y_predicted = classifier.predict(X_test)\n",
    "###\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(\"Majority baseline:\", accuracy_score(y_test, majority_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Probably the biggest jump when moving from traditional linear models with sparse inputs to deep neural networks is to stop representing each feature as a unique dimension, but instead represent them as **dense vectors** (Goldberg, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**discrete representation**\n",
    "\n",
    "$$\\mathbf{x}_{cat} = [0,0,0,0,0,0,1] $$\n",
    "$$\\mathbf{x}_{dog} = [0,0,0,0,1,0,0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**similarity** on discrete representations? $$\\mathbf{x}_{cat} \\wedge \\mathbf{x}_{dog} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>**\"You shall know a word by the company it keeps\"** (Firth, J. R. 1957:11)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/flÃ¸debolle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"The company it keeps\": word co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent the \"company\" of a word in terms of a word co-occurence matrix. On the rows we have the words, on the columns their context.\n",
    "\n",
    "**Contexts** can be of different types, for example:\n",
    "* entire documents\n",
    "* paragraphs\n",
    "* a window around the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\"I like cats\", \"cats like food\", \"dogs hate cats\", \"dogs like food\", \"cats hate dogs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like', 'cats', 'dogs', 'food', 'I', 'hate'}\n"
     ]
    }
   ],
   "source": [
    "vocab = set(np.concatenate([s.split() for s in corpus],0))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2.  1.  2.  1.  0.]\n",
      " [ 2.  0.  2.  1.  1.  2.]\n",
      " [ 1.  2.  0.  1.  0.  2.]\n",
      " [ 2.  1.  1.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  2.  2.  0.  0.  0.]]\n",
      "{'like': 0, 'cats': 1, 'food': 3, 'I': 4, 'hate': 5, 'dogs': 2}\n"
     ]
    }
   ],
   "source": [
    "# lets build a co-occurence matrix (now context is entire document)\n",
    "w2i = {w: i for i,w in enumerate(vocab)}\n",
    "coocurrence_matrix = np.zeros((len(vocab),len(vocab)))\n",
    "for sentence in corpus:\n",
    "    sentence = sentence.split()\n",
    "    for i, word_i in enumerate(sentence):\n",
    "        for j, word_j in enumerate(sentence):\n",
    "            if i!=j:\n",
    "                coocurrence_matrix[(w2i[word_i],w2i[word_j])] +=1\n",
    "print(coocurrence_matrix)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSA (Singular Value Decomposition - SVD)\n",
    "\n",
    "Approximate a matrix $\\mathbf{C}$ through a decomposition into three submatrices:\n",
    "\n",
    "$$\\mathbf{C} \\approx \\mathbf{U \\sum V^T}$$\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/box2.png\">\n",
    "\n",
    "NB. $=$ should be $\\approx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "U, s, V = linalg.svd(coocurrence_matrix)   #alternative:   from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFFFJREFUeJzt3XuMVfW99/H3V7A2IlK0iqhtRU+1tYhc4qioMPXxEeE5\nlqKA0j5Fa7zEtDaNJ7E90QYmlfQYUjxqH0UssWg5UVor4rE+pV5GazEKcgRUUFpAqUXPaYaL4CWo\n3/PHTCccnOG2N7Nn+L1fyQ5rrf3b6/ddK4v92es6kZlIksq0X60LkCTVjiEgSQUzBCSpYIaAJBXM\nEJCkghkCklSwqoRARMyMiLcjYukO2twaESsj4sWIGFiNfiVJlanWnsDdwIj23oyIkcBxmflF4Cpg\nepX6lSRVoCohkJnPAOt30GQ0cE9L2+eAXhHRpxp9S5L2XEedEzgKWLvN+Jst0yRJNeSJYUkqWPcO\n6udN4HPbjB/dMu0TIsKHGUnSbsrM2JPPVXNPIFpebZkHTASIiNOADZn5dnszykxfVXhNmjSp5jXs\nSy/Xp+uzs74qUZU9gYj4N6AeODQi3gAmAZ8CMjNnZOZvI2JURPwJ2AJ8uxr9SpIqU5UQyMxv7EKb\n71ajL0lS9XhieB9WX19f6xL2Ka7P6nJ9dg5R6fGkaouI7Gw1SVJnFhFkJzgxLEnqYgwBSSqYISBJ\nBTMEJKlghoAkFcwQkKSCGQKSVDBDQJIKZghIUsEMAUkqmCEgSQUzBCSpYIaAJBXMEJCkghkCklQw\nQ0CSCmYISFLBDAFJKpghIEkFMwQkqWCGgCQVzBCQpIIZApJUMENAkgpmCEhSwQwBSSqYISBJBTME\nJKlghoAkFcwQkKSCGQKSVDBDQJIKZghIUsEMAUkqmCEgSQUzBNSqZ8+eAKxbt47x48cDMGvWLK65\n5ppaliVpLzIE1CoiAOjbty9z5sz5xHRJ+x5DQJ/w+uuvc9JJJ31i+iOPPMIZZ5xBU1MTf/vb3xg7\ndiynnnoqp556KgsWLKhBpZIq1b3WBahz2v7X/9y5c7n55pt59NFHOfjgg/nmN7/Jtddey9ChQ1m7\ndi0jRozglVdeqVG1kvaUIaCdevzxx1m0aBHz58/noIMOAuCxxx5j+fLlZCYAmzdv5t133+XAAw+s\nZamSdpMhoJ067rjjWL16Na+++ipDhgwBIDN57rnn2H///WtcnaRKeE5Arf7+q357xxxzDA888AAT\nJ05k+fLlAJx77rnccsstrW2WLFnSITVKqi5DQK12dBXQ8ccfz+zZsxk3bhyrV6/mlltuYdGiRZx8\n8sn079+fO++8swMrlVQt0d6vv1qJiOxsNUlSZxYRZOYeXcvtnoAkFawqIRAR50XEioh4LSJ+0Mb7\nwyNiQ0QsbnndUI1+JUmVqTgEImI/4GfACOArwISI+FIbTZ/OzMEtrxsr7Ve11d4NZe156KGHWLFi\nxV6sSNKeqMaeQB2wMjNfz8ytwH3A6Dba+eyBfczuPE5i7ty5vPzyy3uxGkl7ohohcBSwdpvxv7RM\n297pEfFiRDwSESdWoV/V2IcffsiVV15J//79Oe+88/jggw/4+c9/Tl1dHYMGDWLcuHG8//77PPvs\ns8ybN4/rrruOwYMHs3r1alatWsXIkSM55ZRTGD58OK+99lqtF0cqUkfdLPYC8PnMfDciRgJzgePb\nazx58uTW4fr6eurr6/d2fdoDK1eu5P7772fGjBlcdNFFPPDAA1x44YVcfvnlAPzoRz9i5syZfOc7\n3+FrX/sa559/PhdccAEA55xzDnfeeSfHHXcczz//PFdffTWPP/54LRdH6jIaGxtpbGysyryqEQJv\nAp/fZvzolmmtMnPzNsOPRsTtEXFIZja1NcNtQ0Cd17HHHtt6XmDIkCGsWbOGZcuWccMNN7Bhwwa2\nbNnCiBEjPvG5LVu2sGDBAsaNG9d6g9rWrVs7tHapK9v+x3FDQ8Mez6saIbAQ+IeI+AKwDrgYmLBt\ng4jok5lvtwzX0Xx/QpsBoK7jgAMOaB3u1q0b7733Hpdeeinz5s2jf//+zJo1i6eeeuoTn/v444/p\n3bs3ixcv7shyJbWh4nMCmfkR8F1gPvAycF9mLo+IqyLiypZmYyPipYj4D+BfgYsq7Ve119ZNfZs3\nb+aII45g69atzJ49u3V6z5492bRpU+twv379+PWvf936/tKlS/d+wZI+oSr3CWTm/8/MEzLzi5n5\nLy3T7szMGS3D/y8z+2fmoMwcmpnPVaNf1db2VwdFBD/+8Y+pq6vjrLPO4stf/nLrexdffDFTp05l\nyJAhrF69mtmzZzNz5kwGDhxI//79mTdvXkeXLwkfGyFJXZ6PjZAk7RFDQJIKZghIUsEMAUkqmCEg\nSQUzBCSpYIaAJBXMEJCkghkCklQwQ0CSCmYISFLBDAFJKpghIEkFMwQkqWCGgCQVzBCQpIIZApJU\nMENAkgpmCEhSwQwBSSqYISBJBTMEJKlghoAkFcwQkKSCGQKSVDBDQJIKZghIUsEMAUkqmCEgSQUz\nBCSpYIaAJBXMEJCkghkCklQwQ0CSCmYISFLBDAFJKpghIEkFMwQkqWCGgLqknj171roEaZ9gCKhL\niohalyDtEwwBSSqYISBJBTMERENDA9OmTat1GZJqwBCQpIIZAoWaMmUKJ5xwAsOGDePVV18FYMmS\nJZx++ukMHDiQCy+8kI0bNwKwcOFCTj75ZAYPHsx1113HSSedBMArr7zCqaeeyuDBgxk4cCB//vOf\nO6z+zOywvqR9mSFQoMWLFzNnzhyWLl3KI488wsKFC8lMJk6cyNSpU3nxxRfp378/DQ0NAFx22WXc\nddddLF68mG7durVemTN9+nS+//3vs3jxYhYtWsTRRx/dYcvg1UFSdVQlBCLivIhYERGvRcQP2mlz\na0SsjIgXI2JgNfrVnvnDH/7AmDFjOOCAA+jZsyejR49my5YtbNy4kTPPPBOASy65hKeffpqNGzey\nefNm6urqAPjGN77ROp/TTz+dKVOmMHXqVNasWcMBBxzQYcuwadOmDutL2pdVHAIRsR/wM2AE8BVg\nQkR8abs2I4HjMvOLwFXA9Er7VfXs7NBKe+9PmDCBhx9+mE9/+tOMGjWKxsbGvVCdpL2pGnsCdcDK\nzHw9M7cC9wGjt2szGrgHIDOfA3pFRJ8q9K09MGzYMObOncsHH3zAO++8w8MPP0yPHj3o3bs3f/zj\nHwG49957GT58OL169eLggw9m4cKFANx3332t81m9ejX9+vXjmmuuYfTo0SxdurQmyyNpz3WvwjyO\nAtZuM/4XmoNhR23ebJn2dhX6124aNGgQF110EQMGDKBPnz7U1dUREcyaNYurrrqK9957j2OPPZa7\n774bgJkzZ3L55ZfTrVu31mAAmDNnDvfeey/7778/ffv25frrr6/lYknaA1HpVRYRcSEwIjOvbBn/\nv0BdZn5vmzYPAz/JzAUt448B12Xm4jbml5MmTWodr6+vp76+vqIaVZktW7bQo0cPAG666Sbeeust\nbr755hpXJZWrsbHxfxx+bWhoIDP36GqJaoTAacDkzDyvZfyHQGbmTdu0mQ48mZn3t4yvAIZn5if2\nBCIivfyvc5kzZw4/+clP+PDDDznmmGP4xS9+waGHHlrrsiS1iIiahkA34FXgfwHrgOeBCZm5fJs2\no4DvZOb/aQmNf83M09qZnyEgSbuhkhCo+JxAZn4UEd8F5tN8onlmZi6PiKua384ZmfnbiBgVEX8C\ntgDfrrRfSVLlKt4TqDb3BCRp91SyJ+Adw5JUMENAkgpmCEhSwQwBSSqYISBJBTMEVDW33norJ554\nIt/61rcqmk+/fv1oamqqUlWSdqQazw6SALjjjjt4/PHHOfLIIyuaj38rQOo47gmoKq6++mpWrVrF\nyJEjmTZtGmPGjOHkk09m6NChLFu2DID169e3Ob2pqYkRI0Zw0kknccUVV/hXw6QOZAioKu644w6O\nOuoonnzySdasWcPgwYNZsmQJU6ZMYeLEiQBMmjSpzekNDQ2cddZZLFu2jDFjxvDGG2/UclGkong4\nSFWVmTzzzDP85je/AeCrX/0qTU1NvPPOO+1Of/rpp3nwwQcBGDVqFL17965Z/VJp3BNQVbV3PH93\npns4SOo4hoCq5u9f3sOGDeOXv/wl0Pzc889+9rMcdNBBnHXWWW1OHzZsGLNnzwbg0UcfZcOGDbVZ\nAKlAPkBOVXPssceyaNEiIoLLLruMVatW0aNHD2bMmEH//v1Zv359m9ObmpqYMGECf/3rXxk6dCjz\n58/nhRde4JBDDqn1IkldQk3/nkC1GQKStHt8iqgkaY8YApJUMENAkgpmCEhSwQwBSSqYISBJBTME\nJKlghoAkFcwQkKSCGQKSVDBDQJIKZghIUsEMAUkqmCEgSQUzBCSpYIaAJBXMEJCkghkCklQwQ0CS\nCmYISFLBDAFJKpghIEkFMwQkqWCGgCQVzBCQpIIZApJUMENAkgpmCEhSwQwBSSqYISBJBTMEJKlg\nhoAkFcwQkKSCda/kwxHRG7gf+AKwBhifmRvbaLcG2Ah8DGzNzLpK+pUkVUelewI/BB7LzBOAJ4B/\nbqfdx0B9Zg4yACSp86g0BEYDs1qGZwFfb6ddVKEvSVKVVfrFfHhmvg2QmW8Bh7fTLoHfR8TCiLii\nwj4lSVWy03MCEfF7oM+2k2j+Ur+hjebZzmzOyMx1EXEYzWGwPDOfaa/PyZMntw7X19dTX1+/szIl\nqRiNjY00NjZWZV6R2d739i58OGI5zcf6346II4AnM/PLO/nMJOCdzJzWzvtZSU2SVJqIIDNjTz5b\n6eGgecClLcOXAA9t3yAiDoyIg1qGewDnAi9V2K8kqQoq3RM4BJgDfA54neZLRDdERF/grsz8x4jo\nBzxI86Gi7sDszPyXHczTPQFJ2g2V7AlUFAJ7gyEgSbunloeDJEldmCEgSQUzBCSpYIaAJBXMEJCk\nghkCklQwQ0CSCmYIFOKpp57i2WefrXUZkjoZQ6AQjY2NLFiwoNZlSOpkvGO4i7vnnnv46U9/yn77\n7ceAAQMYN24cN954I1u3buXQQw9l9uzZvPvuu5x22ml0796dww47jNtuu41169bR0NBA9+7d6dWr\nV9WeSCip4/nYiEK98sorXHDBBTz77LP07t2bDRs2EBH06tULgJkzZ7JixQqmTp1KQ0MDPXv25Npr\nrwVgwIAB/O53v6Nv375s2rSJgw8+uJaLIqkClYRARX9jWLX1xBNPMG7cOHr37g3AZz7zGV566SXG\njx/PunXr2Lp1K/369Wvzs2eeeSaXXHIJ48eP54ILLujIsiV1Ip4T2Mdcc801fO9732Pp0qVMnz6d\n999/v812t99+O1OmTGHt2rUMGTKE9evXd3ClkjoDQ6ALO/vss/nVr35FU1MTAE1NTWzatIkjjzwS\ngFmzZrW27dmzJ5s2bWodX7VqFaeccgoNDQ0cfvjhrF27tmOLl9QpeDioCzvxxBO5/vrrGT58ON27\nd2fQoEFMnjyZsWPHcsghh3D22WezZs0aAM4//3zGjh3LvHnzuO2225g2bRorV64E4JxzzmHAgAE1\nXBJJteKJYUnq4vx7ApKkPWIISFLBDAFJKpghIEkFMwQkqWCGgCQVzBCQpIIZApJUMENAkgpmCEhS\nwQwBSSqYISBJBTMEJKlghoAkFcwQkKSCGQKSVDBDQJIKZghIUsEMAUkqmCEgSQUzBCSpYIaAJBXM\nEJCkghkCklQwQ0CSCmYISFLBDAFJKpghIEkFMwQkqWCGgCQVrKIQiIixEfFSRHwUEYN30O68iFgR\nEa9FxA8q6VOSVD2V7gksA8YAT7XXICL2A34GjAC+AkyIiC9V2K92QWNjY61L2Ke4PqvL9dk5VBQC\nmflqZq4EYgfN6oCVmfl6Zm4F7gNGV9Kvdo3/yarL9Vldrs/OoSPOCRwFrN1m/C8t0yRJNdZ9Zw0i\n4vdAn20nAQlcn5kP763CJEl7X2Rm5TOJeBL4p8xc3MZ7pwGTM/O8lvEfApmZN7Uzr8oLkqTCZOaO\nDsu3a6d7AruhvQIWAv8QEV8A1gEXAxPam8meLogkafdVeono1yNiLXAa8O8R8WjL9L4R8e8AmfkR\n8F1gPvAycF9mLq+sbElSNVTlcJAkqWuq6R3D3mxWXRHROyLmR8SrEfG7iOjVTrs1EbEkIv4jIp7v\n6Do7u13Z3iLi1ohYGREvRsTAjq6xq9jZuoyI4RGxISIWt7xuqEWdXUVEzIyItyNi6Q7a7Na2WevH\nRnizWXX9EHgsM08AngD+uZ12HwP1mTkoM+s6rLouYFe2t4gYCRyXmV8ErgKmd3ihXcBu/N99OjMH\nt7xu7NAiu567aV6fbdqTbbOmIeDNZlU3GpjVMjwL+Ho77YLa/wDorHZlexsN3AOQmc8BvSKiD9re\nrv7f9WKQXZSZzwDrd9Bkt7fNrvBF4M1mu+7wzHwbIDPfAg5vp10Cv4+IhRFxRYdV1zXsyva2fZs3\n22ijXf+/e3rLoYtHIuLEjiltn7Xb22Y1LxFtkzebVdcO1mdbx1LbO+t/Rmaui4jDaA6D5S2/MKSO\n9gLw+cx8t+VQxlzg+BrXVJS9HgKZ+b8rnMWbwOe3GT+6ZVqRdrQ+W04Y9cnMtyPiCOA/25nHupZ/\n/ysiHqR5t90QaLYr29ubwOd20ka7sC4zc/M2w49GxO0RcUhmNnVQjfua3d42O9PhoJ3ebBYRn6L5\nZrN5HVdWlzIPuLRl+BLgoe0bRMSBEXFQy3AP4FzgpY4qsAvYle1tHjARWu+I3/D3w3D6H3a6Lrc9\nXh0RdTRftm4A7FjQ/vflbm+be31PYEci4uvAbcBnab7Z7MXMHBkRfYG7MvMfM/OjiPj7zWb7ATO9\n2axdNwFzIuIy4HVgPDTfvEfL+qT5UNKDLY/n6A7Mzsz5tSq4s2lve4uIq5rfzhmZ+duIGBURfwK2\nAN+uZc2d1a6sS2BsRFwNbAXeAy6qXcWdX0T8G1APHBoRbwCTgE9RwbbpzWKSVLDOdDhIktTBDAFJ\nKpghIEkFMwQkqWCGgCQVzBCQpIIZApJUMENAkgr237VAZlRanmgzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e0bc1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector for 'like': [-0.43746543  0.55845474 -0.2160615  -0.4472136   0.49799115 -0.04554714]\n",
      "vector for 'cats': [-0.55174508 -0.67046054  0.07651833 -0.4472136  -0.08461001  0.18178592]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "words = list(vocab)\n",
    "for i, lab in enumerate(vocab):\n",
    "    plt.text(U[i,0],U[i,1], words[i])\n",
    "plt.axis([-1, 1, -1, 1])\n",
    "plt.show()\n",
    "\n",
    "print(\"vector for 'like':\", U[w2i[\"like\"]])\n",
    "print(\"vector for 'cats':\", U[w2i[\"cats\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity\n",
    "\n",
    "**cosine** similarity \n",
    "\n",
    "(it ranges from -1 to 1; is 1 if vectors are the same, 0 if they are independent, and -1 if they are exactly opposite)\n",
    "\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/eq1.png\">\n",
    "<img src=\"https://simonpaarlberg.com/posts/2012-06-28-latent-semantic-analyses/vector_example2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: find the most similar words (see similarity.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly learning word vectors (embeddings)\n",
    "\n",
    "* SVD: computation cost scales quadratically with size of co-occurence matrix; difficult to integrate new words\n",
    "* **Idea**: directly learn word vectors (word2vec)\n",
    "    * NLP (almost) from Scratch (Collobert & Weston, 2008)\n",
    "    * word2vec (Mikolov et al, 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main idea of word2vec\n",
    "\n",
    "* instead of capturing co-occurence statistics of words\n",
    "* **predict context** (surrounding words of every word); in particular, predict words in a window of length $m$ around current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$o$ is the outside word (context), $c$ is the current center word; \n",
    "\n",
    "Maximize the probability of a word in the context ($o$) given the current word $c$:\n",
    "\n",
    "$$p(o|c) = \\frac{exp(u_o^T v_c)}{\\sum_{w=1}^W exp(u_w^T v_c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.gabormelli.com/RKB/images/a/a6/skip-gram_NNLM_architecture.150216.jpg\" width=500>\n",
    "\n",
    "NB. denominator $\\sum$ over all words! In practice, *negative sampling* is used (randomly choose a word which is not in context as a negative sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning we represent words as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**a) sparse representation vs b) dense representation**  (Figure 1 in Yoav Goldberg's primer)\n",
    "<img src=\"pics/sparsevsdense.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Traditional vs deep learning approach to feature extraction (representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The common pipeline of extracting features **for an NLP model with a Neural Network** then becomes:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector **for each feature** (lookup table)\n",
    "* **combine** vectors of features to get the vector representation for the **instance** $\\mathbf{x}$ (**dense representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets compare this to our traditional approach - the common pipeline of extracting features for an NLP model is:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector whose length is the total number of features with a 1 at position k if the k-th feature is active; this feature vector represents the **instance** $\\mathbf{x}$  (**sparse representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "Now it should be clear why it is called sparse vs dense feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you combine different feature vector representations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In an NLP application, $\\mathbf{x}$ is usually composed of various embedding vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Following the notation in Goldberg (2015), chapter 4, lets use the function $c(\\cdot)$ as **feature combiner** that creates our input embeddings layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common choice for $c$ is **concatenation**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f_3) = [v(f_1); v(f_2); v(f_3)] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, $c$ could be the **sum of the embeddings vector**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1)+v(f_2)+v(f_3)] $\n",
    "\n",
    "or the **mean**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [mean(v(f_1),v(f_2),v(f_3))] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In many papers $v$ is often referred to as the embeddings layer or lookup layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Our example from before with explicit input representation\n",
    "\n",
    "For instance, let us explicitly state the input representation. Suppose we use the concatentation operator, then our network above becomes:\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "since: \n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "then: \n",
    "\n",
    "$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{[v(f_1); v(f_2); v(f_3)]W^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As computational graph:\n",
    "<img src=\"pics/yg-compgraph2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The values of the *embedding vectors* (values of the vectors in Fig 1 b)) are treated as model parameters and trained together with the other parameters of the model (weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unrolled (graph with concrete input, expected output, and loss node, Goldberg Figure 3 c):\n",
    "<img src=\"pics/yg-compgraph3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, in deep learning approaches to NLP words are represented as dense vectors. Where do these word vectors (embeddings) come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **randomly initialized** (small numbers around 0) and *trained with the network*\n",
    "* **off-the-shelf embeddings**: you can also use already trained, available embeddings (e.g. estimated with *word2vec*) and *initialize* the embedding layer of the network with your pretrained (unsupervised) word embeddings\n",
    "* **task-specific embeddings**: you could also train your embeddings, read them off the network, and use them for another task (or in a multi-task setup, more later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: animacy classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "* Yoav Goldberg's primer chapter 2 and 5: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Simon Paarlberg's [blog on LSA](https://simonpaarlberg.com/post/latent-semantic-analyses/)\n",
    "* Richard Socher's [lecture 2](https://www.youtube.com/watch?v=xhHOL3TNyJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
