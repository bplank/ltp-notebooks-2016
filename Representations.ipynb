{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how a neural network can be formalized, both algebraically and graphically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y= NN(\\mathbf{x}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we have seen last time that the following network can be formalized as:\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what is the input $\\textbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go further, lets make a detour and recap: How do we represent a training instance in a traditional classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, recall our example from week 1: training a Logistic Regression classifier for sentiment classification. \n",
    "\n",
    "* Describe in words: what were the features we used?\n",
    "* Describe how the features of a single training instance got encoded as $\\textbf{x}$\n",
    "* How can you now describe the entire training data set as one matrix $X$ that keeps all training instances?  i.e.,  what are the rows and columns of $X$? $$ X = \\{\\mathbf{x_1}, ... , \\mathbf{x_n}\\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(to be precise, our training data also contain labels, therefore, the entire training data is often also represented as tuples $\\langle \\mathbf{x},\\mathbf{y} \\rangle$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the biggest jump when moving from traditional linear models with sparse inputs to deep neural networks is to stop representing each feature as a unique dimension, but instead represent them as **dense vectors** (Goldberg, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning we usually work with dense representations. Each feature is a vector of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) sparse representation vs b) dense representation**  (Figure 1 in Yoav Goldberg's primer)\n",
    "<img src=\"pics/sparsevsdense.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the *embedding vectors* (values of the vectors in Fig 1 b)) are treated as model parameters and trained together with the other parameters of the model (weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The common pipeline of extracting features **for an NLP model with a Neural Network** then becomes:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector **for each feature** (lookup table)\n",
    "* **combine** vectors of features to get the vector representation for the **instance** $\\mathbf{x}$ (**dense representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare this to our traditional approach - the common pipeline of extracting features for an NLP model is:\n",
    "\n",
    "* extract a set of core linguistic features $f_1,..f_n$\n",
    "* define a vector whose length is the total number of features with a 1 at position k if the k-th feature is active; this feature vector represents the **instance** $\\mathbf{x}$  (**sparse representation**)\n",
    "* use $\\mathbf{x}$ as representation for an instance, train the model\n",
    "\n",
    "Now it should be clear why it is called sparse vs dense feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you combine different feature vector representations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an NLP application, $\\mathbf{x}$ is usually composed of various embedding vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the notation in Goldberg (2015), chapter 4, lets use the function $c(\\cdot)$ as **feature combiner** that creates our input embeddings layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common choice for $c$ is **concatenation**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1); v(f_2); v(f_3)] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, $c$ could be the **sum of the embeddings vector**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1)+v(f_2)+v(f_3)] $\n",
    "\n",
    "or the **mean**:\n",
    "\n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [mean(v(f_1),v(f_2),v(f_3))] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many papers $v$ is often referred to as the embeddings layer or lookup layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let us explicitly state the input representation. Suppose we use the concatentation operator, then our network above becomes:\n",
    "\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "since: \n",
    "$\\mathbf{x} = c(f_1, f_2, f3) = [v(f_1); v(f_2); v(f_3)] $\n",
    "\n",
    "then: \n",
    "\n",
    "$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{[v(f_1); v(f_2); v(f_3)]W^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As computational graph:\n",
    "<img src=\"pics/yg-compgraph2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unrolled (graph with concrete input, expected output, and loss node, Goldberg Figure 3 c):\n",
    "<img src=\"pics/yg-compgraph3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in deep learning approaches to NLP words are represented as dense vectors. Where do these word vectors (embeddings) come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **randomly initialized** (small numbers around 0) and *trained with the network*\n",
    "* **off-the-shelf embeddings**: you can also use already trained, available embeddings (e.g. estimated with *word2vec*) and *initialize* the embedding layer of the network with your pretrained (unsupervised) word embeddings\n",
    "* **task-specific embeddings**: you could also train your embeddings, read them off the network, and use them for another task (or in a multi-task setup, more later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: animacy classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
